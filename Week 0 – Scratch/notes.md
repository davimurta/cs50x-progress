# CS50x – Week 0 (Introduction)

## What Computer Science really is

Computer Science is not about learning programming languages.
Languages are just tools.

The core of Computer Science is **problem solving**:

- understanding a problem
- breaking it into smaller parts
- defining clear steps to solve it

This means that learning how to think is more important than memorizing syntax.

## Key idea: Problem Solving

A computer will only do exactly what you tell it to do.
If the instructions are unclear or incomplete, the result will be wrong.

So the real challenge is not coding,
but expressing solutions in a precise and logical way.

## MIT Fire Hydrant Story

One of the examples shown was the famous MIT fire hydrant hack,
where students connected a fire hydrant to a drinking fountain.

 This illustrates:

- creativity in problem solving
- using existing systems in unexpected ways
- thinking beyond the "intended use" of a tool

## Personal interpretation – Learning perspective

I also interpreted this example from a learning perspective.

It shows that having a large **supply of information** is not enough
if the system (or the person) cannot absorb it at the same pace.

In the context of studying:

- consuming too much content at once leads to shallow understanding
- learning requires time to process and reflect
- progress depends on absorption, not volume

This reinforces the importance of studying at a sustainable pace,
focusing on depth rather than speed.

## Binary

Computers represent data using binary numbers (0 and 1).

Each **bi**nary digi**t** represents a power of 2.
By combining bits, computers can represent numbers, text, and instructions.

## ASCII

ASCII is a standard that maps numbers to characters.

By using numeric codes, computers can represent letters, digits,
and symbols using binary values.

This shows that text is not stored as characters,
but as numbers that are interpreted by a standard.

### Numbers vs Characters – Context matters

While studying ASCII, I had a question about how a system
distinguishes a number from a character.

Computers do not inherently know the difference between a number and a character.
They only store binary values.

For example, the value 64 in memory can represent:

- the number 64, if interpreted as an integer
- the character '@', if interpreted using the ASCII standard

The meaning depends on context:

- the data type
- the program logic
- the convention used to interpret the bits

This helped me better understand why typed programming languages exist.
By explicitly defining data types, the language provides context
about how the stored bits should be interpreted.

This shows that data has no meaning by itself.
Meaning comes from how the system interprets it.
